{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is a basic convnet designed to test out our idea on 13/Feb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the Neural Net structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require \"nn\"\n",
    "require \"torch\"\n",
    "\n",
    "function BasicConvNet1()\n",
    "\t-- Number of filters in different layers \n",
    "\tnet = nn.Sequential()\n",
    "\tHyperColumnHeight = 512\n",
    "\tLayer1FilterNum = 32\n",
    "\tLayer2FilterNum = 64\n",
    "\tLayer3FilterNum = 2\n",
    "\n",
    "\n",
    "\t-- [[ Layer 1 ]]\n",
    "\t-- HyperColumnHeight input image channel, Layer1FilterNum output channels, 3x3 convolution kernel, 1 stride W, 1 stride H, 1 pad W, 1 pad R\n",
    "\tnet:add(nn.SpatialConvolution(HyperColumnHeight, Layer1FilterNum, 3, 3, 1, 1, 1, 1)) \n",
    "\t-- Batch Normalization\n",
    "\tnet:add(nn.SpatialBatchNormalization(Layer1FilterNum))                       \n",
    "\t-- ReLU non-linearity\n",
    "\tnet:add(nn.ReLU())\n",
    "\t-- 2x2 max-pooling\n",
    "\tnet:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "\n",
    "\n",
    "\t-- [[ Layer 2 ]]\n",
    "\t-- Layer1FilterNum input image channel, Layer2FilterNum output channels, 3x3 convolution kernel, 1 stride W, 1 stride H, 1 pad W, 1 pad R\n",
    "\tnet:add(nn.SpatialConvolution(Layer1FilterNum, Layer2FilterNum, 3, 3, 1, 1, 1, 1)) \n",
    "\t-- Batch Normalization\n",
    "\tnet:add(nn.SpatialBatchNormalization(Layer2FilterNum))                       \n",
    "\t-- ReLU non-linearity\n",
    "\tnet:add(nn.ReLU())\n",
    "\t-- 2x2 max-pooling\n",
    "\tnet:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "\n",
    "\t--[[ Layer 3 ]]\n",
    "\t-- Layer2FilterNum input image channel, Layer3FilterNum output channels, 3x3 convolution kernel, 1 stride W, 1 stride H, 1 pad W, 1 pad R\n",
    "\tnet:add(nn.SpatialConvolution(Layer2FilterNum, Layer3FilterNum, 3, 3, 1, 1, 1, 1)) \n",
    "\t-- Adding transfer function sigmod \n",
    "\tnet:add(nn.Sigmoid())  \n",
    "    \n",
    "    net:zeroGradParameters()\n",
    "\treturn net\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: Defining and load the data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- To Load the data into trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all the required parts of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet Loaded\t\n",
       "MSEcriterion defined\t\n",
       "Trainer defined\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Loading the net\n",
    "require('Net1')\n",
    "net = BasicConvNet1()\n",
    "print('ConvNet Loaded')\n",
    "\n",
    "-- Defining a loss function\n",
    "Weight = torch.ones(2,28,28)-- All ones matrix -- Bad coding\n",
    "criterion = nn.WeightedMSECriterion(Weight)\n",
    "print('MSEcriterion defined')\n",
    "\n",
    "trainer = nn.StochasticGradient(net, criterion)\n",
    "-- learning rate init\n",
    "trainer.learningRate = 0.001 --\n",
    "-- Number of Epocs\n",
    "trainer.maxIteration = 5 \n",
    "print('Trainer defined')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check with fake inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No code error during sanity check (Logical error might still exist)\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "-- Passing a random input\n",
    "input = torch.rand(2,512,112,112) -- pass a random tensor as input to the network\n",
    "-- Obtain the output\n",
    "output = net:forward(input)\n",
    "\n",
    "-- Checking the backward pass using a fake expected output\n",
    "ExpectedFakeOutput = torch.rand( output:size() )\n",
    "criterion:forward(output, ExpectedFakeOutput) -- let's say the groundtruth was class number: 3\n",
    "gradients = criterion:backward(output, ExpectedFakeOutput)\n",
    "print(\"No code error during sanity check (Logical error might still exist)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print('The training starts')\n",
    "-- Woah! Traingin the data\n",
    "trainer:train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
